{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of benoit-sentiment-analysis.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "DaoT2xbDDjHH",
        "colab_type": "code",
        "outputId": "579753eb-0257-43a9-b108-1d6e6cd898e3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        }
      },
      "cell_type": "code",
      "source": [
        "! pip install torch"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting torch\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7e/60/66415660aa46b23b5e1b72bc762e816736ce8d7260213e22365af51e8f9c/torch-1.0.0-cp36-cp36m-manylinux1_x86_64.whl (591.8MB)\n",
            "\u001b[K    100% |████████████████████████████████| 591.8MB 27kB/s \n",
            "tcmalloc: large alloc 1073750016 bytes == 0x62162000 @  0x7f63482be2a4 0x591a07 0x5b5d56 0x502e9a 0x506859 0x502209 0x502f3d 0x506859 0x504c28 0x502540 0x502f3d 0x506859 0x504c28 0x502540 0x502f3d 0x506859 0x504c28 0x502540 0x502f3d 0x507641 0x502209 0x502f3d 0x506859 0x504c28 0x502540 0x502f3d 0x507641 0x504c28 0x502540 0x502f3d 0x507641\n",
            "\u001b[?25hInstalling collected packages: torch\n",
            "Successfully installed torch-1.0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "PzseOVNMC8Ne",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "L'analyse de sentiment consiste à prédire à partir d'un texte la polarité du sentiment associé. Le système prend en entrée une séquence de mots et génère en sortie une unique prédiction.\n",
        "\n",
        "Il faut commencer par télécharger les données pour ce problème d'analyse de sentiment.\n",
        "L'archive contient `sanders-twitter-sentiment.csv` qui est un petit corpus de tweets annotés\n",
        "avec une cible (apple, microsoft, google...) et une étiquette de sentiment (positif, negatif, neutre, non pertient)."
      ]
    },
    {
      "metadata": {
        "id": "b4FtuesjC8Nk",
        "colab_type": "code",
        "outputId": "bfc27771-f8d7-41dd-c046-e7f7157eee7a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        }
      },
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "[ -f sanders-twitter-sentiment.csv ] || wget -q http://pageperso.lif.univ-mrs.fr/~benoit.favre/files/sanders-twitter-sentiment.csv\n",
        "head -3 sanders-twitter-sentiment.csv"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\"1044\",\"125667332931596290\",\"2011-10-16 20:20:01\",\"&quot;3 principal global players will be active in every market...@Amazon, @Apple, &amp; @Kobo.&quot; #fbf11 #publaunch @MikeShatzkin http://t.co/1ndxcMO1\",\"neutral\",\"apple\"\r\n",
            "\"71\",\"126384526925639681\",\"2011-10-18 19:49:53\",\"If you've been struggling to get hold of me, I'm back online with a new iPhone - thanks @apple\",\"neutral\",\"apple\"\r\n",
            "\"278\",\"126281019476291585\",\"2011-10-18 12:58:35\",\"@azee1v1 @apple @umber Proper consolidation, proper syncing, stop losing my PURCHASED items, checkboxes that do what you think they will do.\",\"negative\",\"apple\"\r\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "kYfIA_1yC8Nu",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Les colonnes qui nous intéressent dans le csv sont les colonnes 4, 5 et 6 contenant le tweet, l'étiquette et la cible. \n",
        "\n",
        "On peut charger rapidement les données avec la classe python qui lit des csv. Pour faciliter les traitements, nous allons ajouter la cible (`row[5]`) en début de tweet (`row[3]`) entre chevrons. L'étiquette est dans `row[4]`. \n",
        "\n",
        "Pour vérifier que tout s'est bien passé on peut afficher le nombre d'exemples chargés ainsi qu'un exemple arbitrire."
      ]
    },
    {
      "metadata": {
        "id": "G1Gpj-PDC8Nw",
        "colab_type": "code",
        "outputId": "b98f9069-3004-4797-c4fc-093de9528abb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "cell_type": "code",
      "source": [
        "texts = []\n",
        "labels = []\n",
        "\n",
        "import csv\n",
        "with open('sanders-twitter-sentiment.csv', 'r', encoding='utf8') as csvfile:\n",
        "    reader = csv.reader(csvfile, delimiter=',', quotechar='\"')\n",
        "    for row in reader:\n",
        "        texts.append(\"<\" + row[5] + \"> \" + row[3])\n",
        "        labels.append(row[4])\n",
        "\n",
        "print(len(texts))\n",
        "\n",
        "print(texts[12])\n",
        "print(labels[12])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "5513\n",
            "<apple> Wow I am loving this new @apple update for my touch. #coolness Well done\n",
            "positive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "iSPICDj8C8N4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "La premier chose à faire est de créer un dictionnaire pour faire correspondre les étiquettes à des entiers. Les étiquettes ainsi converties seront stockées dans la liste python `int_labels`. "
      ]
    },
    {
      "metadata": {
        "id": "kbARLqvxC8N6",
        "colab_type": "code",
        "outputId": "0294f1be-7f00-4ae8-c30d-af5ba4620376",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "cell_type": "code",
      "source": [
        "label_vocab = {label: i for i, label in enumerate(set(labels))}\n",
        "print(label_vocab)\n",
        "\n",
        "int_labels = [label_vocab[label] for label in labels]\n",
        "print(int_labels[:10])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'negative': 0, 'irrelevant': 1, 'neutral': 2, 'positive': 3}\n",
            "[2, 2, 0, 1, 2, 3, 1, 2, 2, 1]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "EW0aPd2YC8OB",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Nous pouvons opérer de la même manière pour convertir les mots en entiers. Notez que notre système ne fait aucun prétraitement sur le texte du tweet, il se contente de le découper selon les espaces. Un système d'analyse de sentiment plus évolué ferait une tokenisation plus fine, mettrait les mots en minuscues, et irait même jusqu'à les lemmatiser. Notez qu'on utilise un `defaultdict` pour stoquer le vocabulaire. Il attribue un entier à chaque nouveau mot qu'il rencontre. Le mot d'indice 0 est reservé pour un symbole `<eos>` que nous utilserons plus tard. "
      ]
    },
    {
      "metadata": {
        "id": "4Fwxid_VC8OD",
        "colab_type": "code",
        "outputId": "a1b5a30d-fe8b-4b0c-e054-66751b6a2217",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "cell_type": "code",
      "source": [
        "import collections\n",
        "vocab = collections.defaultdict(lambda: len(vocab))\n",
        "vocab['<eos>'] = 0\n",
        "\n",
        "int_texts = []\n",
        "for text in texts:\n",
        "    int_texts.append([vocab[token] for token in text.split()])\n",
        "\n",
        "print(int_texts[12])\n",
        "print(int_labels[12])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1, 180, 181, 182, 183, 96, 33, 37, 184, 124, 46, 185, 186, 187, 188]\n",
            "3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "-BdWIzPtC8OI",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "On peut vérifier que les mots ont bien été convertis en faisant la conversion dans l'autre sens. Il est interessant de regarder la taille du vocabulaire et la taille maximale d'un tweet dans ce corpus."
      ]
    },
    {
      "metadata": {
        "id": "vlXMfO2PC8OL",
        "colab_type": "code",
        "outputId": "36d768b6-7101-4959-e500-bea1d9125571",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "cell_type": "code",
      "source": [
        "rev_vocab = {y: x for x, y in vocab.items()}\n",
        "print([rev_vocab[word_id] for word_id in int_texts[12]])\n",
        "\n",
        "print(len(vocab))\n",
        "print(max([len(text) for text in int_texts]))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['<apple>', 'Wow', 'I', 'am', 'loving', 'this', 'new', '@apple', 'update', 'for', 'my', 'touch.', '#coolness', 'Well', 'done']\n",
            "23541\n",
            "32\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "QxdZPZeyC8OT",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Nous sommes prêts à convertir les données en tenseurs pytorch. Il faut importer les modules de pytorch, et définir quelques constantes qui s'assurent que le problème est de taille raisonnable pour tourner sur CPU. La constante `max_len` est importante car les tweets qui ont une taille superieure à cette dernière seront coupés. "
      ]
    },
    {
      "metadata": {
        "id": "ofQMLmKYC8OW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.autograd import Variable\n",
        "\n",
        "max_len = 16\n",
        "batch_size = 64\n",
        "embed_size = 128\n",
        "hidden_size = 128"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5sjjZfRfC8Od",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Afin de rendre les calculs rapides, il est souhaitable de mettre toutes les données dans des tenseurs. Comme les textes sont de taille variable, nous allons les mettre dans des matrices de taille (nombre de tweets, taille maximum d'un tweet). Les tweets trop grands par rapport à la taille maximale fixée seront coupés, et ceux qui sont plus courts seronts garnis de symboles de padding `<eos>` dont la valeur est 0.\n",
        "\n",
        "Techniquement, il n'est pas nécessaire de mettre toutes les données dans un seul tenseur. En particulier cela devient inefficace quand les textes ont des longueurs très différentes. La seule contrainte est que pour un batch donné, les séquences aient la même taille. De nombreuses bibliothèques (comme `torchtext`) génèrent des batches à la volée qui font la bonne taille.\n",
        "\n",
        "On notera que `textes[12]` a une longeur de 15 symboles et donc `X[12]` se retrouve paddé avec un symbole `<eos>`."
      ]
    },
    {
      "metadata": {
        "id": "yXBOZ09QC8Of",
        "colab_type": "code",
        "outputId": "9898adac-b43c-495a-a375-14d7758f6144",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "cell_type": "code",
      "source": [
        "X = torch.zeros(len(int_texts), max_len).long()\n",
        "\n",
        "for i, text in enumerate(int_texts):\n",
        "    length = min(max_len, len(text))\n",
        "    X[i,:length] = torch.LongTensor(text[:length])\n",
        "\n",
        "Y = torch.LongTensor(int_labels)\n",
        "\n",
        "print(X.size(), Y.size())\n",
        "print(X[12], Y[12])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([5513, 16]) torch.Size([5513])\n",
            "tensor([  1, 180, 181, 182, 183,  96,  33,  37, 184, 124,  46, 185, 186, 187,\n",
            "        188,   0]) tensor(3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ANdnIEKRC8Om",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Pour pouvoir estimer les performances du modèle, nous allons diviser les données en un jeu d'entraînement et un jeu de validation."
      ]
    },
    {
      "metadata": {
        "id": "i4nlmlY4C8Oq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "X_train = X[:5000]\n",
        "Y_train = Y[:5000]\n",
        "X_valid = X[5000:]\n",
        "Y_valid = Y[5000:]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Foc6lHnvC8Ou",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "pytorch fournit un générateur de batches qui s'occupe de mélanger les données. Utilisons le pour nos deux sources de données."
      ]
    },
    {
      "metadata": {
        "id": "uzBlQf_QC8Ov",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "train_set = TensorDataset(X_train, Y_train)\n",
        "valid_set = TensorDataset(X_valid, Y_valid)\n",
        "\n",
        "train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
        "valid_loader = DataLoader(valid_set, batch_size=batch_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hXTuxQG8C8O0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Nous allons d'abord définir une fonction d'évaluation d'un modèle qui calcule le loss moyen sur un ensemble de test, ainsi que le taux d'exemples corrects. Cette fonction utiliser l'entropie croisée comme fonction de loss.\n",
        "\n",
        "Il faut tout d'abord mettre le modèle en mode evaluation pour que les traitements propres à l'apprentissage, comme le dropout, soient désactivés. Il ne faudra pas oublier de le remettre en mode entrainement lors de l'entraînement.\n",
        "\n",
        "Puis pour chaque batch produit par le loader, on peut calculer les scores produits par le modèle pour chaque étiquette, en déduire le loss, et calculer les prédictions en prenant l'indice de la classe de score max.\n",
        "\n",
        "Nous pourrons tester cette fonction lorsque nous aurons créé un modèle."
      ]
    },
    {
      "metadata": {
        "id": "HlrqcmJRC8O2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def perf(model, loader):\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    model.eval()\n",
        "    total_loss = correct = num = 0\n",
        "    for x, y in loader:\n",
        "        y_scores = model(Variable(x, volatile=True))\n",
        "        loss = criterion(y_scores, Variable(y, volatile=True))\n",
        "        y_pred = torch.max(y_scores, 1)[1]\n",
        "        correct += torch.sum(y_pred.data == y)\n",
        "        total_loss += loss.data\n",
        "        num += len(y)\n",
        "    return total_loss / num, float(correct) / float(num)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "lP6GfNuZC8O9",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "La fonction d'apprentissage n'est pas très différente. Elle contient en plus un optimiseur (Adam est utilisé ici car il a des performances raisonnables lorsqu'on a pas encore exploré les hyper-paramètres). Puis pour chaque époque, on n'oublie pas de remettre le modèle en mode entraînement, et cette fois on parcourt les données d'entraînement batch par batch pendant plusieurs époques.\n",
        "\n",
        "L'entraînement nécessite de remettre à zero les accumulateurs de gradient dans le modèle, de calculer les scores prédits pour le batch, d'en déduire la fonction de coût, puis de faire la back-propgation. L'optimiseur peut alors appliquer le gradient aux paramètres du modèle.\n",
        "\n",
        "À la fin de chaque époque, on appelle la fonction `perf` définie précédemment pour se faire une idée des performances sur le jeu de validation."
      ]
    },
    {
      "metadata": {
        "id": "T2PhTkGiC8O_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def fit(model, epochs):\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters())\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        total_loss = num = 0\n",
        "        for x, y in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "            y_scores = model(Variable(x))\n",
        "            loss = criterion(y_scores, Variable(y))\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.data\n",
        "            num += len(y)\n",
        "        print(epoch, total_loss / num, *perf(model, valid_loader))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "BRm_czZkC8PF",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Le premier modèle que nous allons créer est un réseaux de neurones recurrent.\n",
        "Il prend en entrée une séquence d'entiers représentant les mots d'un tweet, puis les projette dans un espace d'embedding. Ces embeddings sont initialement aléatoires et seront appris avec le reste du modèle.\n",
        "Ensuite, il passe par une couche recurrente de type Gated Recurrent Units qui est un peu plus rapide que des LSTM pour des performances similaires. Enfin, l'état caché à la fin de la séquence est utilisé en entrée d'une couche de décision. Cette couche projette l'état caché du RNN vers un espace de dimension le nombre d'étiquettes.\n",
        "\n",
        "Sur GPU, couche recurrente bénéficie d'une accélération grâce à la librairie CuDNN qui nécessite de passer la séquence complète d'entrées plutôt que de manuellement la traiter symbole par symbole (ceci permet de parallèliser plus d'éléments, mais on perd l'acceleration lorsque l'on veut customiser le comportement de cette couche). L'entrée attendue est de taille `(batch_size, sequence_size, embed_size)` si l'on a activé l'option `batch_first`.\n",
        "\n",
        "La couche d'embeddings prend en entrée une matrice de taille `(batch_size, sequence_size)` et renvoie un tenseur de taille `(batch_size, sequence_size, embed_size)`. La taille de l'état caché produit par les couches  RNN de pytorch est `(num_layers * num_directions, batch, hidden_size)`. Donc si on augmente le nombre de couches ou qu'on rend le RNN bidirectionnel, il faudra changer la taille de la couche de sortie en `hidden_size * num_layers * num_directions`. La couche de décision attend en entrée une matrice de taille `(batch_size, in_size)`, donc il faut transposer les deux premières dimensions de `hidden` et la redimensionner (il faut rendre le tenseur contigu pour que pytorch veuille bien en changer la taille)."
      ]
    },
    {
      "metadata": {
        "id": "ynGR4JnnC8PH",
        "colab_type": "code",
        "outputId": "dff269b3-e621-4dd7-f448-a4b77df0fa9f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 159
        }
      },
      "cell_type": "code",
      "source": [
        "class RNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.embed = nn.Embedding(len(vocab), embed_size)\n",
        "        self.rnn = nn.GRU(embed_size, hidden_size, num_layers=1, dropout=0.3, bidirectional=False, batch_first=True)\n",
        "        self.decision = nn.Linear(hidden_size * 1 * 1, len(label_vocab))\n",
        "        \n",
        "    def forward(self, x):\n",
        "        embed = self.embed(x)\n",
        "        output, hidden = self.rnn(embed)\n",
        "        return self.decision(hidden.transpose(0, 1).contiguous().view(x.size(0), -1))\n",
        "\n",
        "rnn_model = RNN()\n",
        "rnn_model"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/rnn.py:46: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  \"num_layers={}\".format(dropout, num_layers))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RNN(\n",
              "  (embed): Embedding(23541, 128)\n",
              "  (rnn): GRU(128, 128, batch_first=True, dropout=0.3)\n",
              "  (decision): Linear(in_features=128, out_features=4, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "metadata": {
        "id": "Ks5Nmt3BC8PK",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "On peut tester le modèle en lui passant comme batch les 3 premiers exemples du corpus encapsulés dans une `Variable`. Le résultat est une matrice de taille `(batch_size, num_labels)`."
      ]
    },
    {
      "metadata": {
        "id": "w8-PFy8_C8PN",
        "colab_type": "code",
        "outputId": "a7310b67-63fc-4e47-8ff4-adbe190a6a9a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "cell_type": "code",
      "source": [
        "rnn_model(Variable(X[:3]))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-0.0222, -0.1528,  0.1297,  0.1454],\n",
              "        [-0.1969,  0.0582, -0.1332, -0.1504],\n",
              "        [ 0.1367, -0.0690, -0.0531,  0.3225]], grad_fn=<AddmmBackward>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "metadata": {
        "id": "LQFFhXVdC8PT",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Nous pouvons alors entraîner le modèle quelques époques avec la fonction `fit`. Il faut vérifier que le loss diminue sur les données d'apprentissage, surveiller le loss sur les données de développement (s'il ne diminue pas, c'est que le modèle ne généralise pas), et les performances calculées par rapport à la métrique que nous intéresse vraiment. Dans la pratique, on sauvegarderait le modèle à chaque fois que les performances sur les données de validation augmentent de manière à ne pas tomber victime de sur-apprentissage. On testerait aussi de nombreux hyper-paramètrages afin de sélectionner le meilleur modèle."
      ]
    },
    {
      "metadata": {
        "scrolled": false,
        "id": "Q8i_Gws3C8PV",
        "colab_type": "code",
        "outputId": "e66b1c1f-8c1e-4952-f8b1-00bad275dd62",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 260
        }
      },
      "cell_type": "code",
      "source": [
        "fit(rnn_model, 10)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:6: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
            "  \n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:7: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
            "  import sys\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "0 tensor(0.0184) tensor(0.0158) 0.5886939571150097\n",
            "1 tensor(0.0127) tensor(0.0131) 0.6647173489278753\n",
            "2 tensor(0.0097) tensor(0.0121) 0.672514619883041\n",
            "3 tensor(0.0072) tensor(0.0123) 0.6978557504873294\n",
            "4 tensor(0.0052) tensor(0.0135) 0.6939571150097466\n",
            "5 tensor(0.0033) tensor(0.0150) 0.6939571150097466\n",
            "6 tensor(0.0019) tensor(0.0163) 0.6666666666666666\n",
            "7 tensor(0.0011) tensor(0.0187) 0.6686159844054581\n",
            "8 tensor(0.0008) tensor(0.0188) 0.6686159844054581\n",
            "9 tensor(0.0005) tensor(0.0202) 0.6881091617933723\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "OtItIbBDC8Pb",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Le second modèle que nous allons créer est un réseau convolutionnel. La convolution permet d'extraire des n-grammes d'embeddings de mots, puis on la suit d'un max-pooling sur la séquence pour que la position des n-grammes soit invariante. La couche Conv1d génère la convolution sur la séquence d'embeddings. Elle attend en entrée un tenseur de taille `(batch_size, embedding_size, sequence_length)`, ce qui va demander de transposer les deux dernières dimensions du tenseur produit par la couche d'embedding. La couche de convolution applique fait un produit entre sa matrice de paramètres et les n-grammes extraits de manière à pouvoir apprendre à sélectionner plusieurs n-grammes. Ses sorties sont de taille `(batch_size, num_filters, sequence_length)`. `kernel_size` permet de régler la taille des n-grammes extraits. La couche de convolution étant linéaire, nous appliquons une fonction non linéaire de type ReLU (rectified linear unit).\n",
        "\n",
        "La deuxième étape est d'appliquer le max pooling. Il existe une couche de max pooling mais elle est plus adaptée au traitement des images, nous allons donc utiliser la fonction `max_pool1d` qui prend en entrée le tenseur produit par la couche de convolution et la fenêtre sur laquelle appliquer le max (ici, toute la longueur de la séquence). Cette couche renvoie un tenseur de taille `(batch_size, num_filters, num_max_windows)` avec `num_max_windows=1` pour nous. On pourrait imaginer un CNN multi-couches qui fasse progressivement le max sur des sous-fenêtres pour obtenir des représentations plus dépendentes de la position des mots. Il suffit ensuite de redimensionner le tenseur produit par la couche de pooling pour le passer à la couche de décision."
      ]
    },
    {
      "metadata": {
        "id": "vkuLt89MC8Pc",
        "colab_type": "code",
        "outputId": "465fe62c-140e-4047-c24e-3ffcaa2b88a4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        }
      },
      "cell_type": "code",
      "source": [
        "class CNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.embed = nn.Embedding(len(vocab), embed_size)\n",
        "        self.conv = nn.Conv1d(embed_size, hidden_size, kernel_size=2)\n",
        "        self.decision = nn.Linear(hidden_size, len(label_vocab))\n",
        "\n",
        "    def forward(self, x):\n",
        "        embed = self.embed(x)\n",
        "        conv = F.relu(self.conv(embed.transpose(1,2)))\n",
        "        pool = F.max_pool1d(conv, conv.size(2))\n",
        "        return self.decision(pool.view(x.size(0), -1))\n",
        "\n",
        "cnn_model = CNN()\n",
        "cnn_model"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CNN(\n",
              "  (embed): Embedding(23541, 128)\n",
              "  (conv): Conv1d(128, 128, kernel_size=(2,), stride=(1,))\n",
              "  (decision): Linear(in_features=128, out_features=4, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "metadata": {
        "id": "jH1bIHuyC8Ph",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "On peut tester que le modèle renvoie bien une matrice de taille `(batch_size, num_labels)`."
      ]
    },
    {
      "metadata": {
        "id": "L88WuerkC8Pm",
        "colab_type": "code",
        "outputId": "0d4eb966-c6c3-4ba4-9e21-13dc000162e0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "cell_type": "code",
      "source": [
        "cnn_model(Variable(X[:3]))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0.8967,  0.9365,  0.8993,  0.1770],\n",
              "        [ 0.4715,  0.9028,  0.8607, -0.0553],\n",
              "        [ 0.3138,  1.0111,  0.8947,  0.2862]], grad_fn=<AddmmBackward>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "metadata": {
        "id": "u-m85USQC8Ps",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Et l'entraîner avec la fonction `fit`."
      ]
    },
    {
      "metadata": {
        "scrolled": false,
        "id": "StrF77b-C8Pw",
        "colab_type": "code",
        "outputId": "b3b6bbcb-f982-4d7f-cada-2bf992a8fcad",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 260
        }
      },
      "cell_type": "code",
      "source": [
        "fit(cnn_model, 10)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:6: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
            "  \n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:7: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
            "  import sys\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "0 tensor(0.0161) tensor(0.0148) 0.6393762183235867\n",
            "1 tensor(0.0115) tensor(0.0131) 0.6900584795321637\n",
            "2 tensor(0.0087) tensor(0.0125) 0.6900584795321637\n",
            "3 tensor(0.0065) tensor(0.0119) 0.7134502923976608\n",
            "4 tensor(0.0045) tensor(0.0117) 0.7173489278752436\n",
            "5 tensor(0.0030) tensor(0.0118) 0.7387914230019493\n",
            "6 tensor(0.0019) tensor(0.0122) 0.7309941520467836\n",
            "7 tensor(0.0012) tensor(0.0125) 0.7407407407407407\n",
            "8 tensor(0.0008) tensor(0.0129) 0.746588693957115\n",
            "9 tensor(0.0006) tensor(0.0132) 0.746588693957115\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "e_EKkEbMC8P0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Exercice\n",
        "---------\n",
        "\n",
        "Créer un modèle RNN+CNN qui donne en entrée du CNN la sortie du RNN au lieu des embeddings. La taille de la sortie du RNN est `(batch, seq_len, hidden_size * num_directions)`.\n",
        "\n",
        "Pour aller plus loin\n",
        "-----------------------\n",
        "- Quel modèle obtient les meilleurs performances ?\n",
        "- Quelles sont les performances d'un RNN avec 2 couches ? bidirectionnel ? \n",
        "- Quel est l'impact de la taille du noyeau pour le CNN ? Que se passe-t-il si l'on change la fonction d'activiation après la convolution ?\n",
        "- En général, on fait des CNN qui extraient des n-grammes de taille 1 à n (par exemple 1, 2 et 3). Pour celà on met en parallèle plusieurs couches de convolution avec des tailles de filtres différentes, et on concatène les représentations créées après max-pooling. La fonction `torch.cat([x1, x2, ...], dim)` permet de concatèner pluseurs tenseurs sur une dimension donnée."
      ]
    },
    {
      "metadata": {
        "scrolled": true,
        "tags": [
          "solution"
        ],
        "id": "VlRdbZ3VC8P1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}