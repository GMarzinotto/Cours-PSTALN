{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of benoit-language-models.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "HaQ0bqCjL7Rw",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "La modélisation du langage consiste à apprendre la distribution de probabilité du mot suivant étant donné un historique. Ici, nous allons créer un modèle de langage sur des caractères pour apprendre à générer des titres de films de science fiction.\n",
        "\n",
        "Le jeu de données provient d'IMDB qui permet d'accèder à de nombreuses infos sur les films, et en plus donne ces données en téléchargement libre (http://www.imdb.com/interfaces/).\n",
        "\n",
        "Le fichier movies-sf.txt contient des noms de films suivis de leur année de sortie entre parenthèses extraits à partir de la base de données IMDB à l'aide de la commande awk en commentaire."
      ]
    },
    {
      "metadata": {
        "id": "P6fYtflrL-2Q",
        "colab_type": "code",
        "outputId": "ec7bdf41-949d-4c45-8b6c-07827fd1e85d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "! pip install torch"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (1.0.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "9Gn3KbQ0L7R0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WcyhOo9qL7R-",
        "colab_type": "code",
        "outputId": "b04dd065-741f-442d-e1f9-c511cb5683c7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        }
      },
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "#wget https://datasets.imdbws.com/title.basics.tsv.gz\n",
        "#zcat title.basics.tsv.gz | awk -F\"\\t\" '$2==\"movie\" && $5==0 && /Sci-Fi/ && $6!=\"\\\\N\"{print $3\" (\"$6\")\"}' | iconv -f utf8 -t ascii//TRANSLIT | sort -u | shuf > movies-sf.txt\n",
        "[ -f movies-sf.txt ] || wget -q http://pageperso.lif.univ-mrs.fr/~benoit.favre/files/movies-sf.txt\n",
        "head movies-sf.txt"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Passengers (2016)\n",
            "Stealth (2005)\n",
            "Utterance (1997)\n",
            "Homunculus, 6. Teil - Das Ende des Homunculus (1917)\n",
            "Framework (2009)\n",
            "Redshift (2013)\n",
            "Jupiter 2023 (2018)\n",
            "Fuerza maldita (1995)\n",
            "Horrors of War (2006)\n",
            "500 MPH Storm (2013)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "VF2WFG8jL7SE",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Nous allons charger les titres caractère par caractère et encoder ces derniers sous forme d'entiers. Le vocabulaire est produit avec un `defaultdict` qui donne un nouvel identifiant à chaque nouveau caractère rencontré. Nous ajoutons deux caractères spéciaux : \n",
        "- le symbole `<eos>` pour le padding\n",
        "- le symbole `<start>` qui indique le début de la séquence\n",
        "\n",
        "Le problème va être posé comme prédire le caractère suivant étant donné le caractère courant et un état caché, et nous avons donc besoin d'un symbole `<start>` pour prédire le premier caractère. La fin d'un texte sera prédite par la première occurrence d'un symbole `<eos>`. \n",
        "\n",
        "Nous pouvons tout de suite créer un vocabulaire inversé pour vérifier le contenu des données chargées."
      ]
    },
    {
      "metadata": {
        "id": "OXaatYjSL7SH",
        "colab_type": "code",
        "outputId": "0426c1ac-03ac-47b8-a2d8-3ced0982d1fa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        }
      },
      "cell_type": "code",
      "source": [
        "import collections\n",
        "vocab = collections.defaultdict(lambda: len(vocab))\n",
        "vocab['<eos>'] = 0\n",
        "vocab['<start>'] = 1\n",
        "\n",
        "int_texts = []\n",
        "with open('movies-sf.txt', 'r') as fp:\n",
        "    for line in fp:\n",
        "        int_texts.append([vocab['<start>']] + [vocab[char] for char in line.strip()])\n",
        "\n",
        "rev_vocab = {y: x for x, y in vocab.items()}\n",
        "\n",
        "print(rev_vocab)\n",
        "print(len(int_texts))\n",
        "\n",
        "print(int_texts[42])\n",
        "print(''.join([rev_vocab[x] for x in int_texts[42]]))"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{0: '<eos>', 1: '<start>', 2: 'P', 3: 'a', 4: 's', 5: 'e', 6: 'n', 7: 'g', 8: 'r', 9: ' ', 10: '(', 11: '2', 12: '0', 13: '1', 14: '6', 15: ')', 16: 'S', 17: 't', 18: 'l', 19: 'h', 20: '5', 21: 'U', 22: 'c', 23: '9', 24: '7', 25: 'H', 26: 'o', 27: 'm', 28: 'u', 29: ',', 30: '.', 31: 'T', 32: 'i', 33: '-', 34: 'D', 35: 'E', 36: 'd', 37: 'F', 38: 'w', 39: 'k', 40: 'R', 41: 'f', 42: '3', 43: 'J', 44: 'p', 45: '8', 46: 'z', 47: 'W', 48: 'M', 49: 'v', 50: 'A', 51: 'N', 52: '4', 53: 'B', 54: 'V', 55: 'I', 56: 'L', 57: 'G', 58: 'b', 59: 'C', 60: ':', 61: 'X', 62: 'x', 63: 'y', 64: 'O', 65: 'Z', 66: 'j', 67: 'q', 68: 'Y', 69: \"'\", 70: '?', 71: 'Q', 72: '/', 73: '&', 74: 'K', 75: '!', 76: '=', 77: '_', 78: '+', 79: ';', 80: '@', 81: '#', 82: '%', 83: '$'}\n",
            "7205\n",
            "[1, 31, 19, 5, 9, 2, 32, 6, 39, 9, 59, 19, 32, 67, 28, 32, 17, 3, 4, 9, 10, 13, 23, 45, 24, 15]\n",
            "<start>The Pink Chiquitas (1987)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "hrVeVD59L7SO",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Afin de bien choisir la longueur maximale sur laquelle le modèle va être entrainé, affichons l'histograme des longueurs de séquences."
      ]
    },
    {
      "metadata": {
        "id": "viewtd04L7SQ",
        "colab_type": "code",
        "outputId": "c4ba0776-c71e-46e6-9df3-9a17ca59ae94",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        }
      },
      "cell_type": "code",
      "source": [
        "from matplotlib import pyplot as plt\n",
        "\n",
        "plt.hist([len(text) for text in int_texts])\n",
        "plt.show()"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD4CAYAAADiry33AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAASUVORK5CYII=\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7fa7d7b9ae10>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "eXjKQvzXL7SV",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Il semble qu'une longeur maximale de 40 permettra de traiter une bonne partie des titres."
      ]
    },
    {
      "metadata": {
        "id": "7-CFO4DEL7SX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.autograd import Variable\n",
        "\n",
        "max_len = 40\n",
        "batch_size = 8\n",
        "embed_size = 16\n",
        "hidden_size = 64"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xqQNx5PyL7Sc",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Le problème est similaire à un problème de tagging sauf que l'étiquette à prédire est le caractère suivant, donc nous devons agencer les tenseurs de manière à ce que $y_{t} = x_{t+1}$. Il faut calculer la longueur après coupure des séquences les plus longues, puis créer un tenseur à partir du texte pour $x$ et un tenseur à partir du texte décalé de 1 vers la gauche pour $y$.\n",
        "\n",
        "N'oublions pas de vérifier que les données ont la bonne forme."
      ]
    },
    {
      "metadata": {
        "id": "ctbYfSVjL7Sh",
        "colab_type": "code",
        "outputId": "ed029f7d-e838-44e8-8533-cc71974f26de",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        }
      },
      "cell_type": "code",
      "source": [
        "X = torch.zeros(len(int_texts), max_len).long()\n",
        "Y = torch.zeros(len(int_texts), max_len).long()\n",
        "\n",
        "for i, text in enumerate(int_texts):\n",
        "    length = min(max_len, len(text) - 1) + 1\n",
        "    X[i,:length - 1] = torch.LongTensor(text[:length - 1])\n",
        "    Y[i,:length - 1] = torch.LongTensor(text[1:length])\n",
        "\n",
        "print(X[42].tolist())\n",
        "print(Y[42].tolist())\n",
        "print([rev_vocab[y] for y in Y[42].tolist()])"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1, 31, 19, 5, 9, 2, 32, 6, 39, 9, 59, 19, 32, 67, 28, 32, 17, 3, 4, 9, 10, 13, 23, 45, 24, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "[31, 19, 5, 9, 2, 32, 6, 39, 9, 59, 19, 32, 67, 28, 32, 17, 3, 4, 9, 10, 13, 23, 45, 24, 15, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "['T', 'h', 'e', ' ', 'P', 'i', 'n', 'k', ' ', 'C', 'h', 'i', 'q', 'u', 'i', 't', 'a', 's', ' ', '(', '1', '9', '8', '7', ')', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "spzoxcZ7L7Sp",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Nous découpons les données en un ensemble d'entraînement et un ensemble de validation, puis les outils pytorch pour créer des batches mélangés sont utilisés comme d'habitude."
      ]
    },
    {
      "metadata": {
        "id": "ehNEE2r4L7Sr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "X_train = X[:6500]\n",
        "Y_train = Y[:6500]\n",
        "X_valid = X[6500:]\n",
        "Y_valid = Y[6500:]\n",
        "\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "train_set = TensorDataset(X_train, Y_train)\n",
        "valid_set = TensorDataset(X_valid, Y_valid)\n",
        "\n",
        "train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
        "valid_loader = DataLoader(valid_set, batch_size=batch_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TXdsUMvoL7Sy",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Le modèle ressemble beaucoup à un taggeur. La première différence est qu'il ne peut pas être bidirectionnel, puisque la causalité est importante (on va générer des textes caractère par caratère en partant de `<start>`). La seconde différence est que la fonction `forward` va prendre un nouveau paramètre optionnel, l'état caché au temps précédent, et renvoyer non seulmenent les scores générés par le modèle, mais le nouvel état caché après avoir vu la séquence représentée dans `x`. Ceci sera nécessire pour la génération caractère par caractère. "
      ]
    },
    {
      "metadata": {
        "id": "pY84K9NkL7Sz",
        "colab_type": "code",
        "outputId": "6b04046d-a7be-4c4c-d78c-bc11ddf105e8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 159
        }
      },
      "cell_type": "code",
      "source": [
        "class LM(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.embed = nn.Embedding(len(vocab), embed_size, padding_idx=vocab['<eos>'])\n",
        "        self.rnn = nn.GRU(embed_size, hidden_size,  bias=False, num_layers=1, dropout=0.3, batch_first=True)\n",
        "        self.decision = nn.Linear(hidden_size, len(vocab))\n",
        "    \n",
        "    def forward(self, x, h_0=None):\n",
        "        embed = self.embed(x)\n",
        "        output, h_n = self.rnn(embed, h_0)\n",
        "        return self.decision(output), h_n\n",
        "\n",
        "model = LM()\n",
        "model"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/rnn.py:46: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  \"num_layers={}\".format(dropout, num_layers))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LM(\n",
              "  (embed): Embedding(84, 16, padding_idx=0)\n",
              "  (rnn): GRU(16, 64, bias=False, batch_first=True, dropout=0.3)\n",
              "  (decision): Linear(in_features=64, out_features=84, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    },
    {
      "metadata": {
        "id": "K-7lqWY0L7S6",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "On peut tester le modèle sur un batch. Il n'est pas obligatoire de passer un état caché initial (le module GRU s'en occupe si l'état caché passé est à `None`), mais on doit récupérer le nouvel état caché même si nous n'allons pas l'utiliser.\n",
        "\n",
        "Remarquons que les sorties sont de taille `(batch_size, sequence_length, num_labels)` et l'état caché `(num_layers, batch_size, hidden_size)`."
      ]
    },
    {
      "metadata": {
        "id": "EtQheQcRL7S8",
        "colab_type": "code",
        "outputId": "a8b4a52d-2582-49d6-8725-ddc655a751cd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "output, hidden = model(Variable(X[:2]))\n",
        "print(output.size(), hidden.size())"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([2, 40, 84]) torch.Size([1, 2, 64])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "DpkP_lhQL7TA",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Il n'y a très peu de différences avec l'évaluation des performances pour un taggeur. Il faut penser que le modèle renvoie maintenant deux résultats (les scores et l'état caché) et donc mettre l'état caché dans une variable qui ne sert à rien.\n",
        "\n",
        "À la place du taux de corrects, nous allons calculer la perplexité du modèle sur les données.\n",
        "\n",
        "$\n",
        "PP(x) = P(x)^{-\\frac{1}{N}} = \\left[\\prod_i P(x_i)\\right]^{-\\frac{1}{N}}\n",
        "$\n",
        "\n",
        "où $x$ est une séquence de mots, $P(x)=\\prod_i P(x_i)$ est la probabilité donnée par le modèle à cette séquence, et $N$ est sa longueur. On peut réécrire ce calcul en domaine log :\n",
        "\n",
        "$\n",
        "PP(x) = exp\\left(-\\frac{1}{N}\\sum_i \\log P(x_i)\\right)\n",
        "$\n",
        "\n",
        "Il se trouve que la fonction de loss renvoie $-\\frac{1}{N}\\log P(x_i)$, donc il suffit de calculer l'exponentielle du loss moyen pour obtenir la perplexité. Cette perplexité n'est pas masquée pour éliminer le padding, donc elle est influencée par ce dernier (on ne pourrait pas profiter de la fonction de loss si l'on souhaitait ignorer le padding)."
      ]
    },
    {
      "metadata": {
        "id": "KHqnyOpcL7TD",
        "colab_type": "code",
        "outputId": "826d8ddc-5b8c-46cc-905c-6ce3c0d5a33c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        }
      },
      "cell_type": "code",
      "source": [
        "import math\n",
        "\n",
        "def perf(model, loader):\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    model.eval()\n",
        "    total_loss = num = 0\n",
        "    for x, y in loader:\n",
        "        y_scores, _ = model(Variable(x, volatile=True))\n",
        "        loss = criterion(y_scores.view(y.size(0) * y.size(1), -1), Variable(y.view(y.size(0) * y.size(1)), volatile=True))\n",
        "        total_loss += loss.data\n",
        "        num += len(y)\n",
        "    return total_loss / num, math.exp(total_loss / num)\n",
        "\n",
        "perf(model, valid_loader)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:8: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
            "  \n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:9: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
            "  if __name__ == '__main__':\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor(0.5639), 1.7575659270313564)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "metadata": {
        "id": "tpf4nPSoL7TH",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "L'apprentissage est le même que pour le taggeur sauf qu'il faut prendre en compte l'état caché."
      ]
    },
    {
      "metadata": {
        "id": "VUGmOAxXL7TK",
        "colab_type": "code",
        "outputId": "30c830df-4cb6-4ab6-f3c4-2b16ed325368",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 260
        }
      },
      "cell_type": "code",
      "source": [
        "def fit(model, epochs):\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters())\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        total_loss = num = 0\n",
        "        for x, y in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "            y_scores, _ = model(Variable(x))\n",
        "            loss = criterion(y_scores.view(y.size(0) * y.size(1), -1), Variable(y.view(y.size(0) * y.size(1))))\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.data\n",
        "            num += len(y)\n",
        "        print(epoch, total_loss / num, *perf(model, valid_loader))\n",
        "\n",
        "fit(model, 10)"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:8: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
            "  \n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:9: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
            "  if __name__ == '__main__':\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "0 tensor(0.1960) tensor(0.1524) 1.1646601669435173\n",
            "1 tensor(0.1463) tensor(0.1423) 1.1529561920424551\n",
            "2 tensor(0.1393) tensor(0.1377) 1.147577896771369\n",
            "3 tensor(0.1353) tensor(0.1348) 1.1442955228140739\n",
            "4 tensor(0.1324) tensor(0.1327) 1.1419126963667787\n",
            "5 tensor(0.1301) tensor(0.1311) 1.1400731864241018\n",
            "6 tensor(0.1283) tensor(0.1301) 1.1389381951866795\n",
            "7 tensor(0.1268) tensor(0.1291) 1.1378011972834516\n",
            "8 tensor(0.1255) tensor(0.1281) 1.136618316543369\n",
            "9 tensor(0.1243) tensor(0.1271) 1.1355317561364742\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "collapsed": true,
        "id": "CKFmuzZAL7TR",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Écrivons maintenant une fonction de génération. Cette dernère créé un tenseur $x$ contenant le symbole `<start>`, et un état caché à 0. Puis, elle repète l'application du modèle sur $x$ et l'état caché, pour générer un nouvel état caché et un vecteur de $y_{\\mathrm{scores}}$ sur les caractères. On peut alors sélectionner la composante de plus haut score, l'afficher et mettre à jour $x$ pour qu'il contienne le symbole généré. Il suffit ensuite de boucler jusqu'à la génération de `<eos>`.\n",
        "\n",
        "Le modèle génère toujours la même séquence de caractères, la séquence la plus probable étant donné le corpus d'apprentissage."
      ]
    },
    {
      "metadata": {
        "id": "E83A7KupL7TT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "outputId": "2cdbd0c0-7921-4314-aed6-a9a74fecc4df"
      },
      "cell_type": "code",
      "source": [
        "#import torch.nn.functional as F\n",
        "def generate_most_probable(model):\n",
        "    x = Variable(torch.zeros((1, 1)).long(), volatile=True)\n",
        "    x[0, 0] = vocab['<start>']\n",
        "    # size for hidden: (batch, num_layers * num_directions, hidden_size)\n",
        "    hidden = Variable(torch.zeros(1, 1, hidden_size), volatile=True)\n",
        "    for i in range(200):\n",
        "        y_scores, hidden = model(x, hidden)\n",
        "        y_pred = torch.max(y_scores, 2)[1]\n",
        "        selected = int(y_pred.data[0, 0].numpy())\n",
        "        if selected == vocab['<eos>']:\n",
        "            break\n",
        "        print(rev_vocab[selected], end='')\n",
        "        x[0, 0] = selected\n",
        "    print()\n",
        "\n",
        "generate_most_probable(model)"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The Man (2017)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:2: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
            "  \n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:5: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
            "  \"\"\"\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "HcofTUhsL7TX",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Exercice\n",
        "---------\n",
        "\n",
        "Plutôt que de sélectionner le caractère ayant la plus grande probabilité, on peut tirer aléatoirement un caractère dans la distribution de probabilité après softmax. Utilisez `F.softmax` et `torch.multinomial` pour tirer aléatoirement un élément $s \\sim softmax(y_{\\textrm{scores}})$ dans la distribution des scores, et l'utiliser comme élément sélectionné à la place de celui issu du $max$.\n",
        "\n",
        "On peut diviser les scores par une température $\\theta$ avant de faire le softmax pour tasser la distriution. Une valeur de $\\theta<1$ poussera le modèle à prendre moins de risque et générer des caractères plus probables, alors que $\\theta>1$ lui fera prendre plus de risques et générer des séquences moins probables. En général, $\\theta=0.7$ donne des résultas satisfaisants.\n",
        "\n",
        "Générez 100 séquences avec cette méthode.\n",
        "\n",
        "Pour aller plus loin\n",
        "-----------------------\n",
        "\n",
        "- Implementez Beam Search pour decoder les sequences en sortie. Ceci pour les cas avec et sans temperature"
      ]
    }
  ]
}